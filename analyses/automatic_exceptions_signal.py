import osimport torchfrom torch.utils.data import DataLoaderimport matplotlib.pyplot as pltimport seaborn as snsimport networksfrom configs import Configimport datafrom plots import prep_axesfrom components import entropy, Sphericaldef plot_violin(value, label, ylabel, title='', save=False, ax=None):    labels_unique = torch.sort(torch.unique(label))[0].numpy()    value_per_class = [value[label==i] for i in labels_unique]    fig, ax = prep_axes(ax)    ax.violinplot(value_per_class, labels_unique, showmeans=True)    #plot trained exceptions separately    ax.scatter([1,1,1], value[label==1][:3], color='r', marker='x')    ax.set_xticks(labels_unique)    ax.set_xlabel('Class')    ax.set_ylabel(ylabel)    ax.set_title(title)    fig.set_size_inches([6.4 , 7.12])    fig.tight_layout()    if save:        fig.savefig(f'violin_{save}.png', pad_inches=0, transparent=True)    return fig, axdef plot_strip(value, label, ylabel, title='', save=False, ax=None):    fig, ax = prep_axes(ax)    sns.stripplot(label.numpy(), value.numpy(),                  x='Class', y='Converge time',                  size=2, jitter=0.45, alpha=0.05, ax=ax, zorder=0)    ax.set_xlabel('Class')    ax.set_ylabel(ylabel)    ax.set_title(title)    fig.set_size_inches([6.4 , 7.12])    fig.tight_layout()    if save:        fig.savefig(f'stripplot_{save}.png', pad_inches=0, transparent=True)    return fig, ax#%%root = '/Users/danil/My/School/Columbia/Research/assoc_mem/results/cluster_mountpoint/2022-11-28/AssociativeMNIST_Exceptions_0001'data_config = Config({        'class': data.AssociativeMNIST,        'perturb_entries': 0.5,        'perturb_mask': 'rand',        'perturb_value': 'min',        'include_test': False,        'num_samples': None,        'select_classes': 'all',        'n_per_class': 'all',        'crop': False,        'downsample': False,        'normalize': True,        })train_data, test_data = data.get_data(**data_config)train_loader = DataLoader(train_data, batch_size=512)labels = train_data[:][3]labels_unique = torch.sort(torch.unique(labels))[0]# MODE = 'converge_time'# MODE = 'hidden'MODE = 'relative_update'# MODE = 'init_distance'#%%rg = False #False, 'unweighted_loss',els = 100 #1, 10, 100, 2000for beta in [10]: #1,10    cfg_label = f'beta={beta} train_beta=False rescale_grads={rg} exception_loss_scaling={els}'    path = os.path.join(root, cfg_label, 'checkpoints')    ckpt = next(os.walk(path))[-1][0]    path = os.path.join(path, ckpt)    net = networks.ExceptionsMHN.load_from_checkpoint(path,                                                      input_nonlin=Spherical())    net.max_steps = 100    #%%    print(MODE)    if MODE == 'relative_update':        update_1_list = []        update_10_list = []        update_100_list = []        for i,batch in enumerate(train_loader):            print(f'batch {i}')            with torch.no_grad():                state_trajectory = net(batch[0], clamp_mask=~batch[2], return_mode='trajectory') #([B,N], [B,M])                output, hidden = [torch.stack(layer_tr) for layer_tr in zip(*state_trajectory)]            update_1_list.append( (batch[0]-output[1]).norm(dim=-1) )            update_10_list.append( (batch[0]-output[10]).norm(dim=-1) )            update_100_list.append( (batch[0]-output[100]).norm(dim=-1) )        update_1 = torch.cat(update_1_list)        update_10 = torch.cat(update_10_list)        update_100 = torch.cat(update_100_list)        #plot        for X_dist, ylabel in zip([update_1, update_10, update_100,],                                  ['Traj len t=1', 'Traj len t=10', 'Traj len t=100']):            ax = None            for plot in [plot_strip, plot_violin]:                fig, ax = plot(X_dist, labels, ylabel, title=f'beta={net.hidden.nonlin.beta}', ax=ax)            fig.savefig(f"{ylabel.lower().replace(' ', '_')}_beta={net.hidden.nonlin.beta}.png", pad_inches=0, transparent=True)#%%    elif MODE == 'init_distance':        min_dist_list = []        avg_dist_list = []        for i,batch in enumerate(train_loader):            print(f'batch {i}')            input = batch[0]            with torch.no_grad():                dist = (input.unsqueeze(0)-net.fc.weight.unsqueeze(1)).norm(dim=-1)            min_dist_list.append( dist.min(dim=0)[0] )            avg_dist_list.append( dist.mean(dim=0) )        min_dist = torch.cat(min_dist_list)        avg_dist = torch.cat(avg_dist_list)        #plot        for X_dist, ylabel in zip([min_dist, avg_dist],                                  ['Min distance', 'Avg distance']):            ax = None            for plot in [plot_strip, plot_violin]:                fig, ax = plot(X_dist, labels, ylabel, title=f'beta={net.hidden.nonlin.beta}', ax=ax)            # fig.savefig(f"{ylabel.lower().replace(' ', '_')}_beta={net.hidden.nonlin.beta}.png", pad_inches=0, transparent=True)#%%    elif MODE == 'hidden':        # output_list = []        hidden_max_list = []        hidden_entropy_list = []        hidden_norm_list = []        for i,batch in enumerate(train_loader):            print(f'batch {i}')            with torch.no_grad():                output, hidden = net(batch[0], clamp_mask=~batch[2]) #([B,N], [B,M])            hidden_max_list.append(hidden.max(dim=1)[0])            hidden_entropy_list.append(entropy(hidden))            hidden_norm_list.append(hidden.norm(dim=1))        hidden_max = torch.cat(hidden_max_list) #[D]        hidden_entropy = torch.cat(hidden_entropy_list)        hidden_norm = torch.cat(hidden_norm_list)        # hidden_per_class = [hidden[labels==i] for i in labels_unique]        d = {'hidden_max':hidden_max, 'hidden_entropy':hidden_entropy, 'hidden_norm':hidden_norm,             'label':labels, 'net_path':path}        torch.save(d, f'outputs_beta={beta}.pt')        #plot        for hidden_X, ylabel in zip([hidden_max, hidden_norm, hidden_entropy],                                    ['Hidden max', 'Hidden norm', 'Hidden entropy']):            ax = None            for plot in [plot_strip, plot_violin]:                fig, ax = plot(hidden_X, labels, ylabel, title=f'beta={net.hidden.nonlin.beta}', ax=ax)            # fig.savefig(f"{ylabel.lower().replace(' ', '_')}_beta={net.hidden.nonlin.beta}.png", pad_inches=0, transparent=True)#%%    elif MODE == 'converge_time':        try: #load            d = torch.load(f'converge_time_per_class_beta={beta}.pt')            net = networks.ExceptionsMHN.load_from_checkpoint(d['net_path'])            converge_time_per_class = d['converge_time_per_class']            converge_time = torch.cat(converge_time_per_class)            labels = torch.repeat_interleave(torch.tensor(labels_unique), torch.tensor([len(ct) for ct in converge_time_per_class]))        except: #compute            train_loader = DataLoader(train_data, batch_size=256)            net.max_steps = 2000            converge_time_list = []            for i,batch in enumerate(train_loader):                print(f'batch {i}')                output, converge_time = net(batch[0], clamp_mask=~batch[2], return_mode='converge_time')                converge_time_list.append(converge_time)            converge_time = torch.cat(converge_time_list)            converge_time_per_class = [converge_time[labels==i] for i in labels_unique]            d = {'converge_time_per_class': converge_time_per_class, 'net_path': path}            torch.save(d, f'converge_time_per_class_beta={beta}.pt')        plot_violin(converge_time, labels, 'Converge time')#,                    # save=f'converge_time_beta={net.hidden.nonlin.beta}')        plot_strip(converge_time, labels, 'Converge time')#,                    # save=f'converge_time_beta={net.hidden.nonlin.beta}')#%%